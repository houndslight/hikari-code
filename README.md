# ğŸŒ¸ Hikari â€” Lightweight Coding Chatbot

**Hikari** is a free and open-source coding assistant built for people with older or lower-end computers.  
It helps you **write, fix, and explain code** using modern open models that run locally â€” no internet required once installed.

This project was created to make AI-powered development **accessible everywhere** ğŸŒ.

---

## ğŸ§  Features

- ğŸ’» Runs entirely **offline** using [Ollama](https://ollama.com)
- âš¡ Works fast even on low-VRAM GPUs or CPUs
- ğŸª¶ Simple, minimalist Japanese-inspired web UI
- ğŸ§© Supports small coding models like:
  - `phi3:mini` (Microsoft â€” very efficient)
  - `codellama:7b-instruct`
  - `deepseek-coder:1.3b-instruct`

---

## ğŸ§° Requirements

Before starting, make sure you have:

| Tool | Description | Download |
|------|--------------|-----------|
| **Python 3.10+** | Needed for the backend (FastAPI) | [python.org/downloads](https://www.python.org/downloads/) |
| **Node.js 18+** | Needed for the frontend build | [nodejs.org](https://nodejs.org/) |
| **Ollama** | Runs the AI model locally | [ollama.com/download](https://ollama.com/download) |

ğŸ–¥ï¸ Recommended minimum hardware:  
**Ryzen 5 / i5 CPU**, 8 GB RAM, optional GPU (4 GB VRAM or higher).

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Clone or download Hikari

```bash
git clone https://github.com/YOURUSERNAME/hikari.git
cd hikari
```

---

### 2ï¸âƒ£ Install the backend (AI server)

```bash
cd backend
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```

Then start the server:

```bash
python -m uvicorn main:app --host 0.0.0.0 --port 8080
```

If it works, youâ€™ll see:

```
âœ“ Ollama is available
Uvicorn running on http://0.0.0.0:8080
```

---

### 3ï¸âƒ£ Install Ollama and pull a model

Go to [https://ollama.com/download](https://ollama.com/download) and install it.

Then open a terminal and type:

```bash
ollama pull phi3:mini
```

ğŸŸ¢ *Phi-3 Mini* is small, fast, and great for writing code.

---

### 4ï¸âƒ£ Build the web UI (frontend)

```bash
cd ../frontend
npm install
npm run build
```

When it finishes, youâ€™ll see a new folder called `dist/`.

---

### 5ï¸âƒ£ Open the chatbot

Go back to the backend folder and start Hikari again:

```bash
cd ../backend
python -m uvicorn main:app --host 0.0.0.0 --port 8080
```

Now open your browser and visit:

ğŸ‘‰ **http://localhost:8080**

You should see your Hikari chat window appear ğŸŒ¸  
Type something like:

> write me a hello world in python

---

## ğŸ§© Common Problems & Fixes

| Problem | Cause | Fix |
|----------|--------|-----|
| âŒ `sh : command not found` | You tried to run Linux install commands on Windows. | Use the **Windows Ollama installer** from the [download page](https://ollama.com/download). |
| âš ï¸ `No backends available!` | Ollama isnâ€™t running or no model is pulled. | Run `ollama pull phi3:mini`, then restart the backend. |
| âŒ `uvicorn: command not found` | FastAPI server not installed in the virtual environment. | Run `pip install fastapi uvicorn httpx` inside `backend/venv`. |
| âš ï¸ `Static directory not found` | You havenâ€™t built the frontend yet. | Run `npm run build` inside `frontend/`. |
| ğŸ§± `EPERM: operation not permitted, stat 'postcss-nested'` | Windows permission lock on `node_modules`. | Close VS Code, delete `node_modules`, run `npm install` again as Administrator. |
| ğŸ˜µ "Sorry, I encountered an error" in web UI | Frontend expected JSON but backend sent a stream. | Make sure your backend has the **non-streaming fix** enabled (see docs below). |
| ğŸ¢ Everything is slow | Running a big model on CPU. | Try `phi3:mini` instead of `codellama:7b-instruct`. |

---

## ğŸ§  Tips for Low-Spec PCs

- Use `phi3:mini` or `tinyllama:1.1b` for best performance.  
- Keep the **FastAPI window open** while using Hikari.  
- Close browsers/tabs to free up RAM.  
- To move models to another drive:  
  ```powershell
  mklink /D "C:\Users\<YourUser>\.ollama\models" "D:\LLMs\models"
  ```

---

## ğŸ§© Folder Structure

```
hikari/
 â”œâ”€â”€ backend/         # FastAPI + Ollama server
 â”‚   â”œâ”€â”€ main.py
 â”‚   â”œâ”€â”€ requirements.txt
 â”œâ”€â”€ frontend/        # Web UI built with Vite
 â”‚   â”œâ”€â”€ dist/
 â”‚   â”œâ”€â”€ package.json
 â””â”€â”€ README.md
```

---

## ğŸ’¡ Future Updates

- Real-time typing (streaming responses)  
- More supported models  
- Offline installer package  
- UI themes (Rose Pine / Tokyo Night)

---

## ğŸª´ License

Released under the **MIT License**.  
Youâ€™re free to use, modify, and share â€” just credit **Houndslight / Hikari**.

---

### ğŸŒ¸ Created with love by [Houndslight](https://houndslight.online)
> â€œFrom us, to you â€” harness your empty desires.â€
